{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [

    
    {
      "cell_type": "markdown",
      "source": [
        "### Arxiv Paper downloader\n"
      ],
      "metadata": {
        "id": "Waa0IDD6heUa"
      }
    },
    
      "metadata": {
        "id": "6H8tbzmKzhr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : Run the cell to install the requirements\n",
        "\n"
      ],
      "metadata": {
        "id": "TPTsLfr3pAXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv-dl feedparser arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy2W-sVeg9RC",
        "outputId": "d5f05c35-4318-4ae1-b4ef-fb5621bbac8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv-dl in /usr/local/lib/python3.10/dist-packages (1.1.4)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.10)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (1.4.8)\n",
            "Requirement already satisfied: colorlog>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from arxiv-dl) (6.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from arxiv-dl) (2.31.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from arxiv-dl) (1.10.13)\n",
            "Requirement already satisfied: pypdf>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from arxiv-dl) (3.16.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from arxiv-dl) (4.11.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->arxiv-dl) (2.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->arxiv-dl) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->arxiv-dl) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->arxiv-dl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->arxiv-dl) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->arxiv-dl) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Run the script -> Give it your query -> set the number of results you want using `` 'max_results': 30`` on line #31 -> Read the title and summaries to skim through papers  \n",
        "\n",
        "Results are shown in markdown format for better readability"
      ],
      "metadata": {
        "id": "TI8JCZyrpoRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GFYgmTFcfXj4",
        "outputId": "5943c462-8616-45aa-9faa-901ae0a147a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the search keyword: Segment anything\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 1) Title: Empirical Evaluation of the Segment Anything Model (SAM) for Brain Tumor\n####   Segmentation\n\n*Brain tumor segmentation presents a formidable challenge in the field of Medical Image Segmentation. While deep-learning models have been useful, human expert segmentation remains the most accurate method. The recently\nreleased Segment Anything Model (SAM) has opened up the opportunity to apply foundation models to this difficult task. However, SAM was primarily trained on diverse natural images. This makes applying SAM to biomedical\nsegmentation, such as brain tumors with less defined boundaries, challenging. In this paper, we enhanced SAM's mask decoder using transfer learning with the Decathlon brain tumor dataset. We developed three methods to\nencapsulate the four-dimensional data into three dimensions for SAM. An on-the-fly data augmentation approach has been used with a combination of rotations and elastic deformations to increase the size of the training\ndataset. Two key metrics: the Dice Similarity Coefficient (DSC) and the Hausdorff Distance 95th Percentile (HD95), have been applied to assess the performance of our segmentation models. These metrics provided valuable\ninsights into the quality of the segmentation results. In our evaluation, we compared this improved model to two benchmarks: the pretrained SAM and the widely used model, nnUNetv2. We find that the improved SAM shows\nconsiderable improvement over the pretrained SAM, while nnUNetv2 outperformed the improved SAM in terms of overall segmentation accuracy. Nevertheless, the improved SAM demonstrated slightly more consistent results than\nnnUNetv2, especially on challenging cases that can lead to larger Hausdorff distances. In the future, more advanced techniques can be applied in order to further improve the performance of SAM on brain tumor\nsegmentation.*\n\nURL: http://arxiv.org/abs/2310.06162v1\n\nAuthors: Mohammad Peivandi, Jason Zhang, Michael Lu, Dongxiao Zhu, Zhifeng Kou\n\n_**Published on**_ : **October 09, 2023 - 09:22 PM**\n\nUpdated on: October 09, 2023 - 09:22 PM\n\n*arXiv ID*: **2310.06162v1**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 2) Title: Segment Anything Model is a Good Teacher for Local Feature Learning\n\n*Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in \"any scene\" and \"any downstream task\". Data-driven local feature\nlearning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM\n(segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary\ntask of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local\nfeature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning Based on Semantic Grouping (WSC), which utilizes semantic groupings derived from SAM as\nweakly supervised signals, to optimize the metric space of local descriptors. Third, we design an Edge Attention Guidance (EAG) to further improve the accuracy of local feature detection and description by prompting the\nnetwork to pay more attention to the edge region guided by SAM. SAMFeat's performance on various tasks such as image matching on HPatches, and long-term visual localization on Aachen Day-Night showcases its superiority\nover previous local features. The release code is available at https://github.com/vignywang/SAMFeat.*\n\nURL: http://arxiv.org/abs/2309.16992v1\n\nAuthors: Jingqian Wu, Rongtao Xu, Zach Wood-Doughty, Changwei Wang\n\n_**Published on**_ : **September 29, 2023 - 05:29 AM**\n\nUpdated on: September 29, 2023 - 05:29 AM\n\n*arXiv ID*: **2309.16992v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 3) Title: nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance\n\n*The recent developments of foundation models in computer vision, especially the Segment Anything Model (SAM), allow scalable and domain-agnostic image segmentation to serve as a general-purpose segmentation tool. In\nparallel, the field of medical image segmentation has benefited significantly from specialized neural networks like the nnUNet, which is trained on domain-specific datasets and can automatically configure the network to\ntailor to specific segmentation challenges. To combine the advantages of foundation models and domain-specific models, we present nnSAM, which synergistically integrates the SAM model with the nnUNet model to achieve\nmore accurate and robust medical image segmentation. The nnSAM model leverages the powerful and robust feature extraction capabilities of SAM, while harnessing the automatic configuration capabilities of nnUNet to\npromote dataset-tailored learning. Our comprehensive evaluation of nnSAM model on different sizes of training samples shows that it allows few-shot learning, which is highly relevant for medical image segmentation where\nhigh-quality, annotated data can be scarce and costly to obtain. By melding the strengths of both its predecessors, nnSAM positions itself as a potential new benchmark in medical image segmentation, offering a tool that\ncombines broad applicability with specialized efficiency. The code is available at https://github.com/Kent0n-Li/Medical-Image-Segmentation.*\n\nURL: http://arxiv.org/abs/2309.16967v2\n\nAuthors: Yunxiang Li, Bowen Jing, Zihan Li, Jing Wang, You Zhang\n\n_**Published on**_ : **September 29, 2023 - 04:26 AM**\n\nUpdated on: October 02, 2023 - 06:45 PM\n\n*arXiv ID*: **2309.16967v2**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 4) Title: NOC: High-Quality Neural Object Cloning with 3D Lifting of Segment\n####   Anything\n\n*With the development of the neural field, reconstructing the 3D model of a target object from multi-view inputs has recently attracted increasing attention from the community. Existing methods normally learn a neural\nfield for the whole scene, while it is still under-explored how to reconstruct a certain object indicated by users on-the-fly. Considering the Segment Anything Model (SAM) has shown effectiveness in segmenting any 2D\nimages, in this paper, we propose Neural Object Cloning (NOC), a novel high-quality 3D object reconstruction method, which leverages the benefits of both neural field and SAM from two aspects. Firstly, to separate the\ntarget object from the scene, we propose a novel strategy to lift the multi-view 2D segmentation masks of SAM into a unified 3D variation field. The 3D variation field is then projected into 2D space and generates the\nnew prompts for SAM. This process is iterative until convergence to separate the target object from the scene. Then, apart from 2D masks, we further lift the 2D features of the SAM encoder into a 3D SAM field in order to\nimprove the reconstruction quality of the target object. NOC lifts the 2D masks and features of SAM into the 3D neural field for high-quality target object reconstruction. We conduct detailed experiments on several\nbenchmark datasets to demonstrate the advantages of our method. The code will be released.*\n\nURL: http://arxiv.org/abs/2309.12790v1\n\nAuthors: Xiaobao Wei, Renrui Zhang, Jiarui Wu, Jiaming Liu, Ming Lu, Yandong Guo, Shanghang Zhang\n\n_**Published on**_ : **September 22, 2023 - 11:02 AM**\n\nUpdated on: September 22, 2023 - 11:02 AM\n\n*arXiv ID*: **2309.12790v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 5) Title: Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow\n####   removal\n\n*Segment Anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came\nto distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to\nachieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention\nGuidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image\nrestoration performance.*\n\nURL: http://arxiv.org/abs/2309.11715v1\n\nAuthors: Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao\n\n_**Published on**_ : **September 21, 2023 - 01:35 AM**\n\nUpdated on: September 21, 2023 - 01:35 AM\n\n*arXiv ID*: **2309.11715v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 6) Title: Segment Anything Model for Brain Tumor Segmentation\n\n*Glioma is a prevalent brain tumor that poses a significant health risk to individuals. Accurate segmentation of brain tumor is essential for clinical diagnosis and treatment. The Segment Anything Model(SAM), released by\nMeta AI, is a fundamental model in image segmentation and has excellent zero-sample generalization capabilities. Thus, it is interesting to apply SAM to the task of brain tumor segmentation. In this study, we evaluated\nthe performance of SAM on brain tumor segmentation and found that without any model fine-tuning, there is still a gap between SAM and the current state-of-the-art(SOTA) model.*\n\nURL: http://arxiv.org/abs/2309.08434v1\n\nAuthors: Peng Zhang, Yaping Wang\n\n_**Published on**_ : **September 15, 2023 - 02:33 PM**\n\nUpdated on: September 15, 2023 - 02:33 PM\n\n*arXiv ID*: **2309.08434v1**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 7) Title: SAMUS: Adapting Segment Anything Model for Clinically-Friendly and\n####   Generalizable Ultrasound Image Segmentation\n\n*Segment anything model (SAM), an eminent universal image segmentation model, has recently gathered considerable attention within the domain of medical image segmentation. Despite the remarkable performance of SAM on\nnatural images, it grapples with significant performance degradation and limited generalization when confronted with medical images, particularly with those involving objects of low contrast, faint boundaries, intricate\nshapes, and diminutive sizes. In this paper, we propose SAMUS, a universal model tailored for ultrasound image segmentation. In contrast to previous SAM-based universal models, SAMUS pursues not only better\ngeneralization but also lower deployment cost, rendering it more suitable for clinical applications. Specifically, based on SAM, a parallel CNN branch is introduced to inject local features into the ViT encoder through\ncross-branch attention for better medical image segmentation. Then, a position adapter and a feature adapter are developed to adapt SAM from natural to medical domains and from requiring large-size inputs (1024x1024) to\nsmall-size inputs (256x256) for more clinical-friendly deployment. A comprehensive ultrasound dataset, comprising about 30k images and 69k masks and covering six object categories, is collected for verification.\nExtensive comparison experiments demonstrate SAMUS's superiority against the state-of-the-art task-specific models and universal foundation models under both task-specific evaluation and generalization evaluation.\nMoreover, SAMUS is deployable on entry-level GPUs, as it has been liberated from the constraints of long sequence encoding. The code, data, and models will be released at https://github.com/xianlin7/SAMUS.*\n\nURL: http://arxiv.org/abs/2309.06824v1\n\nAuthors: Xian Lin, Yangyang Xiang, Li Zhang, Xin Yang, Zengqiang Yan, Li Yu\n\n_**Published on**_ : **September 13, 2023 - 09:15 AM**\n\nUpdated on: September 13, 2023 - 09:15 AM\n\n*arXiv ID*: **2309.06824v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 8) Title: SAM3D: Segment Anything Model in Volumetric Medical Images\n\n*Image segmentation is a critical task in medical image analysis, providing valuable information that helps to make an accurate diagnosis. In recent years, deep learning-based automatic image segmentation methods have\nachieved outstanding results in medical images. In this paper, inspired by the Segment Anything Model (SAM), a foundation model that has received much attention for its impressive accuracy and powerful generalization\nability in 2D still image segmentation, we propose a SAM3D that targets at 3D volumetric medical images and utilizes the pre-trained features from the SAM encoder to capture meaningful representations of input images.\nDifferent from other existing SAM-based volumetric segmentation methods that perform the segmentation by dividing the volume into a set of 2D slices, our model takes the whole 3D volume image as input and processes it\nsimply and effectively that avoids training a significant number of parameters. Extensive experiments are conducted on multiple medical image datasets to demonstrate that our network attains competitive results compared\nwith other state-of-the-art methods in 3D medical segmentation tasks while being significantly efficient in terms of parameters.*\n\nURL: http://arxiv.org/abs/2309.03493v1\n\nAuthors: Nhat-Tan Bui, Dinh-Hieu Hoang, Minh-Triet Tran, Ngan Le\n\n_**Published on**_ : **September 07, 2023 - 06:05 AM**\n\nUpdated on: September 07, 2023 - 06:05 AM\n\n*arXiv ID*: **2309.03493v1**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 9) Title: SAM-Deblur: Let Segment Anything Boost Image Deblurring\n\n*Image deblurring is a critical task in the field of image restoration, aiming to eliminate blurring artifacts. However, the challenge of addressing non-uniform blurring leads to an ill-posed problem, which limits the\ngeneralization performance of existing deblurring models. To solve the problem, we propose a framework SAM-Deblur, integrating prior knowledge from the Segment Anything Model (SAM) into the deblurring task for the first\ntime. In particular, SAM-Deblur is divided into three stages. First, We preprocess the blurred images, obtain image masks via SAM, and propose a mask dropout method for training to enhance model robustness. Then, to\nfully leverage the structural priors generated by SAM, we propose a Mask Average Pooling (MAP) unit specifically designed to average SAM-generated segmented areas, serving as a plug-and-play component which can be\nseamlessly integrated into existing deblurring networks. Finally, we feed the fused features generated by the MAP Unit into the deblurring model to obtain a sharp image. Experimental results on the RealBlurJ, ReloBlur,\nand REDS datasets reveal that incorporating our methods improves NAFNet's PSNR by 0.05, 0.96, and 7.03, respectively. Code will be available at \\href{https://github.com/HPLQAQ/SAM-Deblur}{SAM-Deblur}.*\n\nURL: http://arxiv.org/abs/2309.02270v1\n\nAuthors: Siwei Li, Mingxuan Liu, Yating Zhang, Shu Chen, Haoxiang Li, Hong Chen, Zifei Dou\n\n_**Published on**_ : **September 05, 2023 - 02:33 PM**\n\nUpdated on: September 05, 2023 - 02:33 PM\n\n*arXiv ID*: **2309.02270v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 10) Title: Adapting Segment Anything Model for Change Detection in HR Remote\n####   Sensing Images\n\n*Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct\nuse in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging characteristics of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve\nthe change detection of high-resolution Remote Sensing Images (RSIs). We employ the visual encoder of FastSAM, an efficient variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on\nsome specific ground objects in the RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we\nintroduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods and exhibits a sample-efficient\nlearning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs for the CD of HR RSIs.*\n\nURL: http://arxiv.org/abs/2309.01429v1\n\nAuthors: Lei Ding, Kun Zhu, Daifeng Peng, Hao Tang, Haitao Guo\n\n_**Published on**_ : **September 04, 2023 - 08:23 AM**\n\nUpdated on: September 04, 2023 - 08:23 AM\n\n*arXiv ID*: **2309.01429v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 11) Title: Enhancing Bloodstain Analysis Through AI-Based Segmentation: Leveraging\n####   Segment Anything Model for Crime Scene Investigation\n\n*Bloodstain pattern analysis plays a crucial role in crime scene investigations by providing valuable information through the study of unique blood patterns. Conventional image analysis methods, like Thresholding and\nContrast, impose stringent requirements on the image background and is labor-intensive in the context of droplet image segmentation. The Segment Anything Model (SAM), a recently proposed method for extensive image\nrecognition, is yet to be adequately assessed for its accuracy and efficiency on bloodstain image segmentation. This paper explores the application of pre-trained SAM and fine-tuned SAM on bloodstain image segmentation\nwith diverse image backgrounds. Experiment results indicate that both pre-trained and fine-tuned SAM perform the bloodstain image segmentation task with satisfactory accuracy and efficiency, while fine-tuned SAM achieves\nan overall 2.2\\% accuracy improvement than pre-trained SAM and 4.70\\% acceleration in terms of speed for image recognition. Analysis of factors that influence bloodstain recognition is carried out. This research\ndemonstrates the potential application of SAM on bloodstain image segmentation, showcasing the effectiveness of Artificial Intelligence application in criminology research. We release all code and demos at\n\\url{https://github.com/Zdong104/Bloodstain_Analysis_Ai_Tool}*\n\nURL: http://arxiv.org/abs/2308.13979v1\n\nAuthors: Zihan Dong, ZhengDong Zhang\n\n_**Published on**_ : **August 27, 2023 - 01:11 AM**\n\nUpdated on: August 27, 2023 - 01:11 AM\n\n*arXiv ID*: **2308.13979v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 12) Title: Zero-Shot Edge Detection with SCESAME: Spectral Clustering-based\n####   Ensemble for Segment Anything Model Estimation\n\n*This paper proposes a novel zero-shot edge detection with SCESAME, which stands for Spectral Clustering-based Ensemble for Segment Anything Model Estimation, based on the recently proposed Segment Anything Model (SAM).\nSAM is a foundation model for segmentation tasks, and one of the interesting applications of SAM is Automatic Mask Generation (AMG), which generates zero-shot segmentation masks of an entire image. AMG can be applied to\nedge detection, but suffers from the problem of overdetecting edges. Edge detection with SCESAME overcomes this problem by three steps: (1) eliminating small generated masks, (2) combining masks by spectral clustering,\ntaking into account mask positions and overlaps, and (3) removing artifacts after edge detection. We performed edge detection experiments on two datasets, BSDS500 and NYUDv2. Although our zero-shot approach is simple,\nthe experimental results on BSDS500 showed almost identical performance to human performance and CNN-based methods from seven years ago. In the NYUDv2 experiments, it performed almost as well as recent CNN-based methods.\nThese results indicate that our method has the potential to be a strong baseline for future zero-shot edge detection methods. Furthermore, SCESAME is not only applicable to edge detection, but also to other downstream\nzero-shot tasks.*\n\nURL: http://arxiv.org/abs/2308.13779v1\n\nAuthors: Hiroaki Yamagiwa, Yusuke Takase, Hiroyuki Kambe, Ryosuke Nakamoto\n\n_**Published on**_ : **August 26, 2023 - 06:19 AM**\n\nUpdated on: August 26, 2023 - 06:19 AM\n\n*arXiv ID*: **2308.13779v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 13) Title: SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge\n####   for Semi-Supervised Learning in Medical Image Segmentation\n\n*The Segment Anything Model (SAM) exhibits a capability to segment a wide array of objects in natural images, serving as a versatile perceptual tool for various downstream image segmentation tasks. In contrast, medical\nimage segmentation tasks often rely on domain-specific knowledge (DSK). In this paper, we propose a novel method that combines the segmentation foundation model (i.e., SAM) with domain-specific knowledge for reliable\nutilization of unlabeled images in building a medical image segmentation model. Our new method is iterative and consists of two main stages: (1) segmentation model training; (2) expanding the labeled set by using the\ntrained segmentation model, an unlabeled set, SAM, and domain-specific knowledge. These two stages are repeated until no more samples are added to the labeled set. A novel optimal-matching-based method is developed for\ncombining the SAM-generated segmentation proposals and pixel-level and image-level DSK for constructing annotations of unlabeled images in the iterative stage (2). In experiments, we demonstrate the effectiveness of our\nproposed method for breast cancer segmentation in ultrasound images, polyp segmentation in endoscopic images, and skin lesion segmentation in dermoscopic images. Our work initiates a new direction of semi-supervised\nlearning for medical image segmentation: the segmentation foundation model can be harnessed as a valuable tool for label-efficient segmentation learning in medical image segmentation.*\n\nURL: http://arxiv.org/abs/2308.13759v1\n\nAuthors: Yizhe Zhang, Tao Zhou, Shuo Wang, Ye Wu, Pengfei Gu, Danny Z. Chen\n\n_**Published on**_ : **August 26, 2023 - 04:46 AM**\n\nUpdated on: August 26, 2023 - 04:46 AM\n\n*arXiv ID*: **2308.13759v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 14) Title: SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene\n####   Reconstruction by Neural Radiance Field (NeRF)\n\n*The accurate reconstruction of surgical scenes from surgical videos is critical for various applications, including intraoperative navigation and image-guided robotic surgery automation. However, previous approaches,\nmainly relying on depth estimation, have limited effectiveness in reconstructing surgical scenes with moving surgical tools. To address this limitation and provide accurate 3D position prediction for surgical tools in\nall frames, we propose a novel approach called SAMSNeRF that combines Segment Anything Model (SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates accurate segmentation masks of surgical tools using\nSAM, which guides the refinement of the dynamic surgical scene reconstruction by NeRF. Our experimental results on public endoscopy surgical videos demonstrate that our approach successfully reconstructs high-fidelity\ndynamic surgical scenes and accurately reflects the spatial information of surgical tools. Our proposed approach can significantly enhance surgical navigation and automation by providing surgeons with accurate 3D\nposition information of surgical tools during surgery.The source code will be released soon.*\n\nURL: http://arxiv.org/abs/2308.11774v1\n\nAuthors: Ange Lou, Yamin Li, Xing Yao, Yike Zhang, Jack Noble\n\n_**Published on**_ : **August 22, 2023 - 08:31 PM**\n\nUpdated on: August 22, 2023 - 08:31 PM\n\n*arXiv ID*: **2308.11774v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 15) Title: SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT\n\n*The Segment Anything Model (SAM) has gained significant attention in the field of image segmentation due to its impressive capabilities and prompt-based interface. While SAM has already been extensively evaluated in\nvarious domains, its adaptation to retinal OCT scans remains unexplored. To bridge this research gap, we conduct a comprehensive evaluation of SAM and its adaptations on a large-scale public dataset of OCTs from RETOUCH\nchallenge. Our evaluation covers diverse retinal diseases, fluid compartments, and device vendors, comparing SAM against state-of-the-art retinal fluid segmentation methods. Through our analysis, we showcase adapted\nSAM's efficacy as a powerful segmentation model in retinal OCT scans, although still lagging behind established methods in some circumstances. The findings highlight SAM's adaptability and robustness, showcasing its\nutility as a valuable tool in retinal OCT image analysis and paving the way for further advancements in this domain.*\n\nURL: http://arxiv.org/abs/2308.09331v2\n\nAuthors: Botond Fazekas, José Morano, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović\n\n_**Published on**_ : **August 18, 2023 - 06:26 AM**\n\nUpdated on: August 31, 2023 - 07:45 AM\n\n*arXiv ID*: **2308.09331v2**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 16) Title: CEmb-SAM: Segment Anything Model with Condition Embedding for Joint\n####   Learning from Heterogeneous Datasets\n\n*Automated segmentation of ultrasound images can assist medical experts with diagnostic and therapeutic procedures. Although using the common modality of ultrasound, one typically needs separate datasets in order to\nsegment, for example, different anatomical structures or lesions with different levels of malignancy. In this paper, we consider the problem of jointly learning from heterogeneous datasets so that the model can improve\ngeneralization abilities by leveraging the inherent variability among datasets. We merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. We propose to train a single\nsegmentation model so that the model can adapt to each sub-group. For robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. We\npropose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. The conditional embedding block effectively adapts SAM to each image sub-group by\nincorporating dataset properties through learnable parameters for normalization. Experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer.\nThe experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.*\n\nURL: http://arxiv.org/abs/2308.06957v1\n\nAuthors: Dongik Shin, Beomsuk Kim, Seungjun Baek\n\n_**Published on**_ : **August 14, 2023 - 06:22 AM**\n\nUpdated on: August 14, 2023 - 06:22 AM\n\n*arXiv ID*: **2308.06957v1**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 17) Title: Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot\n####   Segment Anything Model for Molecular-empowered Learning\n\n*Precise identification of multiple cell classes in high-resolution Giga-pixel whole slide imaging (WSI) is critical for various clinical scenarios. Building an AI model for this purpose typically requires pixel-level\nannotations, which are often unscalable and must be done by skilled domain experts (e.g., pathologists). However, these annotations can be prone to errors, especially when distinguishing between intricate cell types\n(e.g., podocytes and mesangial cells) using only visual inspection. Interestingly, a recent study showed that lay annotators, when using extra immunofluorescence (IF) images for reference (referred to as molecular-\nempowered learning), can sometimes outperform domain experts in labeling. Despite this, the resource-intensive task of manual delineation remains a necessity during the annotation process. In this paper, we explore the\npotential of bypassing pixel-level delineation by employing the recent segment anything model (SAM) on weak box annotation in a zero-shot learning approach. Specifically, we harness SAM's ability to produce pixel-level\nannotations from box annotations and utilize these SAM-generated labels to train a segmentation model. Our findings show that the proposed SAM-assisted molecular-empowered learning (SAM-L) can diminish the labeling\nefforts for lay annotators by only requiring weak box annotations. This is achieved without compromising annotation accuracy or the performance of the deep learning-based segmentation. This research represents a\nsignificant advancement in democratizing the annotation process for training pathological image segmentation, relying solely on non-expert annotators.*\n\nURL: http://arxiv.org/abs/2308.05785v1\n\nAuthors: Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo\n\n_**Published on**_ : **August 10, 2023 - 04:44 PM**\n\nUpdated on: August 10, 2023 - 04:44 PM\n\n*arXiv ID*: **2308.05785v1**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 18) Title: Adaptive Low Rank Adaptation of Segment Anything to Salient Object\n####   Detection\n\n*Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and Google's PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model\n(SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to\ndetect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient\nobject detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across five challenging RGB benchmark datasets demonstrate the superior performance\nof our approach, surpassing state-of-the-art methods.*\n\nURL: http://arxiv.org/abs/2308.05426v1\n\nAuthors: Ruikai Cui, Siyuan He, Shi Qiu\n\n_**Published on**_ : **August 10, 2023 - 08:39 AM**\n\nUpdated on: August 10, 2023 - 08:39 AM\n\n*arXiv ID*: **2308.05426v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 19) Title: SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment\n####   Anything Model\n\n*Optical Flow Estimation aims to find the 2D dense motion field between two frames. Due to the limitation of model structures and training datasets, existing methods often rely too much on local clues and ignore the\nintegrity of objects, resulting in fragmented motion estimation. Through theoretical analysis, we find the pre-trained large vision models are helpful in optical flow estimation, and we notice that the recently famous\nSegment Anything Model (SAM) demonstrates a strong ability to segment complete objects, which is suitable for solving the fragmentation problem. We thus propose a solution to embed the frozen SAM image encoder into\nFlowFormer to enhance object perception. To address the challenge of in-depth utilizing SAM in non-segmentation tasks like optical flow estimation, we propose an Optical Flow Task-Specific Adaption scheme, including a\nContext Fusion Module to fuse the SAM encoder with the optical flow context encoder, and a Context Adaption Module to adapt the SAM features for optical flow task with Learned Task-Specific Embedding. Our proposed\nSAMFlow model reaches 0.86/2.10 clean/final EPE and 3.55/12.32 EPE/F1-all on Sintel and KITTI-15 training set, surpassing Flowformer by 8.5%/9.9% and 13.2%/16.3%. Furthermore, our model achieves state-of-the-art\nperformance on the Sintel and KITTI-15 benchmarks, ranking #1 among all two-frame methods on Sintel clean pass.*\n\nURL: http://arxiv.org/abs/2307.16586v3\n\nAuthors: Shili Zhou, Ruian He, Weimin Tan, Bo Yan\n\n_**Published on**_ : **July 31, 2023 - 11:40 AM**\n\nUpdated on: August 16, 2023 - 04:53 AM\n\n*arXiv ID*: **2307.16586v3**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 20) Title: Industrial Segment Anything -- a Case Study in Aircraft Manufacturing,\n####   Intralogistics, Maintenance, Repair, and Overhaul\n\n*Deploying deep learning-based applications in specialized domains like the aircraft production industry typically suffers from the training data availability problem. Only a few datasets represent non-everyday objects,\nsituations, and tasks. Recent advantages in research around Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions. As recently\ndemonstrated by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety. Although, investigating its\napplication within specific domains is subject to ongoing research. This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases. We include manufacturing, intralogistics, as\nwell as maintenance, repair, and overhaul processes, also representing a variety of other neighboring industrial domains. Besides presenting the various use cases, we further discuss the injection of domain knowledge.*\n\nURL: http://arxiv.org/abs/2307.12674v1\n\nAuthors: Keno Moenck, Arne Wendt, Philipp Prünte, Julian Koch, Arne Sahrhage, Johann Gierecker, Ole Schmedemann, Falko Kähler, Dirk Holst, Martin Gomse, Thorsten Schüppstuhl, Daniel Schoepflin\n\n_**Published on**_ : **July 24, 2023 - 10:24 AM**\n\nUpdated on: July 24, 2023 - 10:24 AM\n\n*arXiv ID*: **2307.12674v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 21) Title: Open Scene Understanding: Grounded Situation Recognition Meets Segment\n####   Anything for Helping People with Visual Impairments\n\n*Grounded Situation Recognition (GSR) is capable of recognizing and interpreting visual scenes in a contextually intuitive way, yielding salient activities (verbs) and the involved entities (roles) depicted in images. In\nthis work, we focus on the application of GSR in assisting people with visual impairments (PVI). However, precise localization information of detected objects is often required to navigate their surroundings confidently\nand make informed decisions. For the first time, we propose an Open Scene Understanding (OpenSU) system that aims to generate pixel-wise dense segmentation masks of involved entities instead of bounding boxes.\nSpecifically, we build our OpenSU system on top of GSR by additionally adopting an efficient Segment Anything Model (SAM). Furthermore, to enhance the feature extraction and interaction between the encoder-decoder\nstructure, we construct our OpenSU system using a solid pure transformer backbone to improve the performance of GSR. In order to accelerate the convergence, we replace all the activation functions within the GSR decoders\nwith GELU, thereby reducing the training duration. In quantitative analysis, our model achieves state-of-the-art performance on the SWiG dataset. Moreover, through field testing on dedicated assistive technology datasets\nand application demonstrations, the proposed OpenSU system can be used to enhance scene understanding and facilitate the independent mobility of people with visual impairments. Our code will be available at\nhttps://github.com/RuipingL/OpenSU.*\n\nURL: http://arxiv.org/abs/2307.07757v1\n\nAuthors: Ruiping Liu, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ke Cao, Yufan Chen, Kailun Yang, Rainer Stiefelhagen\n\n_**Published on**_ : **July 15, 2023 - 09:41 AM**\n\nUpdated on: July 15, 2023 - 09:41 AM\n\n*arXiv ID*: **2307.07757v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 22) Title: SAM-Path: A Segment Anything Model for Semantic Segmentation in Digital\n####   Pathology\n\n*Semantic segmentations of pathological entities have crucial clinical value in computational pathology workflows. Foundation models, such as the Segment Anything Model (SAM), have been recently proposed for universal use\nin segmentation tasks. SAM shows remarkable promise in instance segmentation on natural images. However, the applicability of SAM to computational pathology tasks is limited due to the following factors: (1) lack of\ncomprehensive pathology datasets used in SAM training and (2) the design of SAM is not inherently optimized for semantic segmentation tasks. In this work, we adapt SAM for semantic segmentation by introducing trainable\nclass prompts, followed by further enhancements through the incorporation of a pathology encoder, specifically a pathology foundation model. Our framework, SAM-Path enhances SAM's ability to conduct semantic segmentation\nin digital pathology without human input prompts. Through experiments on two public pathology datasets, the BCSS and the CRAG datasets, we demonstrate that the fine-tuning with trainable class prompts outperforms vanilla\nSAM with manual prompts and post-processing by 27.52% in Dice score and 71.63% in IOU. On these two datasets, the proposed additional pathology foundation model further achieves a relative improvement of 5.07% to 5.12%\nin Dice score and 4.50% to 8.48% in IOU.*\n\nURL: http://arxiv.org/abs/2307.09570v1\n\nAuthors: Jingwei Zhang, Ke Ma, Saarthak Kapse, Joel Saltz, Maria Vakalopoulou, Prateek Prasanna, Dimitris Samaras\n\n_**Published on**_ : **July 12, 2023 - 08:15 PM**\n\nUpdated on: July 12, 2023 - 08:15 PM\n\n*arXiv ID*: **2307.09570v1**\n\nPrimary Category:  _**eess.IV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 23) Title: SAM-IQA: Can Segment Anything Boost Image Quality Assessment?\n\n*Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on\npre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a\nrecently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been\nshown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive\nexperiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively\nand quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code:\nhttps://github.com/Hedlen/SAM-IQA*\n\nURL: http://arxiv.org/abs/2307.04455v1\n\nAuthors: Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu\n\n_**Published on**_ : **July 10, 2023 - 10:07 AM**\n\nUpdated on: July 10, 2023 - 10:07 AM\n\n*arXiv ID*: **2307.04455v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 24) Title: Segment Anything Meets Point Tracking\n\n*The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks. This paper presents SAM-PT, a method extending\nSAM's capability to tracking and segmenting anything in dynamic videos. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation\ntracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional object-centric mask propagation strategies, we uniquely\nuse point propagation to exploit local structure information that is agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video\nObjects (UVO) benchmark. To further enhance our approach, we utilize K-Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object. We also employ\nmultiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy. Our code integrates different point trackers and video segmentation benchmarks and will be\nreleased at https://github.com/SysCV/sam-pt.*\n\nURL: http://arxiv.org/abs/2307.01197v1\n\nAuthors: Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu\n\n_**Published on**_ : **July 03, 2023 - 05:58 PM**\n\nUpdated on: July 03, 2023 - 05:58 PM\n\n*arXiv ID*: **2307.01197v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 25) Title: SAMAug: Point Prompt Augmentation for Segment Anything Model\n\n*This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide\nmore information to SAM. From the initial point prompt, SAM produces the initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate\naugmented segmentation masks based on the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We evaluate four point augmentation techniques: random selection, maximum\ndifference entropy, maximum distance, and a saliency model. Experiments on the COCO, Fundus, and Chest X-ray datasets demonstrate that SAMAug can boost SAM's segmentation results, especially using the maximum distance\nand saliency model methods. SAMAug underscores the potential of visual prompt engineering to advance interactive computer vision models.*\n\nURL: http://arxiv.org/abs/2307.01187v1\n\nAuthors: Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei Liu, Quanzheng Li, Tianming Liu, Xiang Li\n\n_**Published on**_ : **July 03, 2023 - 05:52 PM**\n\nUpdated on: July 03, 2023 - 05:52 PM\n\n*arXiv ID*: **2307.01187v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 26) Title: RefSAM: Efficiently Adapting Segmenting Anything Model for Referring\n####   Video Object Segmentation\n\n*The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for\nprecise user-interactive prompts and a limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which explores the potential of SAM for RVOS by incorporating\nmulti-view information from diverse modalities and successive frames at different timestamps in an online manner. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a\nlightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Additionally, we have introduced the hierarchical dense\nattention module to fuse hierarchical visual semantic information with sparse embeddings in order to obtain fine-grained dense embeddings, and an implicit tracking module to generate a track token and provide historical\ninformation for the mask decoder. Furthermore, we employ a parameter-efficient tuning strategy to effectively align and fuse the language and vision features. Through comprehensive ablation studies, we demonstrate the\npractical and effective design choices of our model. Extensive experiments conducted on Ref-Youtu-VOS, Ref-DAVIS17, and three referring image segmentation datasets validate the superiority and effectiveness of our RefSAM\nmodel over existing methods. The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.*\n\nURL: http://arxiv.org/abs/2307.00997v2\n\nAuthors: Yonglin Li, Jing Zhang, Xiao Teng, Long Lan\n\n_**Published on**_ : **July 03, 2023 - 01:21 PM**\n\nUpdated on: October 02, 2023 - 02:32 AM\n\n*arXiv ID*: **2307.00997v2**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 27) Title: Topological Data Analysis Guided Segment Anything Model Prompt\n####   Optimization for Zero-Shot Segmentation in Biological Imaging\n\n*Emerging foundation models in machine learning are models trained on vast amounts of data that have been shown to generalize well to new tasks. Often these models can be prompted with multi-modal inputs that range from\nnatural language descriptions over images to point clouds. In this paper, we propose topological data analysis (TDA) guided prompt optimization for the Segment Anything Model (SAM) and show preliminary results in the\nbiological image segmentation domain. Our approach replaces the standard grid search approach that is used in the original implementation and finds point locations based on their topological significance. Our results\nshow that the TDA optimized point cloud is much better suited for finding small objects and massively reduces computational complexity despite the extra step in scenarios which require many segmentations.*\n\nURL: http://arxiv.org/abs/2306.17400v1\n\nAuthors: Ruben Glatt, Shusen Liu\n\n_**Published on**_ : **June 30, 2023 - 05:00 AM**\n\nUpdated on: June 30, 2023 - 05:00 AM\n\n*arXiv ID*: **2306.17400v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 28) Title: Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and\n####   Localization\n\n*The rapid advancements in computer vision have stimulated remarkable progress in face forgery techniques, capturing the dedicated attention of researchers committed to detecting forgeries and precisely localizing\nmanipulated areas. Nonetheless, with limited fine-grained pixel-wise supervision labels, deepfake detection models perform unsatisfactorily on precise forgery detection and localization. To address this challenge, we\nintroduce the well-trained vision segmentation foundation model, i.e., Segment Anything Model (SAM) in face forgery detection and localization. Based on SAM, we propose the Detect Any Deepfakes (DADF) framework with the\nMultiscale Adapter, which can capture short- and long-range forgery contexts for efficient fine-tuning. Moreover, to better identify forged traces and augment the model's sensitivity towards forgery regions,\nReconstruction Guided Attention (RGA) module is proposed. The proposed framework seamlessly integrates end-to-end forgery localization and detection optimization. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of our approach for both forgery detection and localization. The codes will be released soon at https://github.com/laiyingxin2/DADF.*\n\nURL: http://arxiv.org/abs/2306.17075v1\n\nAuthors: Yingxin Lai, Zhiming Luo, Zitong Yu\n\n_**Published on**_ : **June 29, 2023 - 04:25 PM**\n\nUpdated on: June 29, 2023 - 04:25 PM\n\n*arXiv ID*: **2306.17075v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 29) Title: The Segment Anything Model (SAM) for Remote Sensing Applications: From\n####   Zero to One Shot\n\n*Segmentation is an essential step for remote sensing image processing. This study aims to advance the application of the Segment Anything Model (SAM), an innovative image segmentation model by Meta AI, in the field of\nremote sensing image analysis. SAM is known for its exceptional generalization capabilities and zero-shot learning, making it a promising approach to processing aerial and orbital images from diverse geographical\ncontexts. Our exploration involved testing SAM across multi-scale datasets using various input prompts, such as bounding boxes, individual points, and text descriptors. To enhance the model's performance, we implemented\na novel automated technique that combines a text-prompt-derived general example with one-shot training. This adjustment resulted in an improvement in accuracy, underscoring SAM's potential for deployment in remote\nsensing imagery and reducing the need for manual annotation. Despite the limitations encountered with lower spatial resolution images, SAM exhibits promising adaptability to remote sensing data analysis. We recommend\nfuture research to enhance the model's proficiency through integration with supplementary fine-tuning techniques and other networks. Furthermore, we provide the open-source code of our modifications on online\nrepositories, encouraging further and broader adaptations of SAM to the remote sensing domain.*\n\nURL: http://arxiv.org/abs/2306.16623v1\n\nAuthors: Lucas Prado Osco, Qiusheng Wu, Eduardo Lopes de Lemos, Wesley Nunes Gonçalves, Ana Paula Marques Ramos, Jonathan Li, José Marcato Junior\n\n_**Published on**_ : **June 29, 2023 - 01:49 AM**\n\nUpdated on: June 29, 2023 - 01:49 AM\n\n*arXiv ID*: **2306.16623v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n#### 30) Title: Let Segment Anything Help Image Dehaze\n\n*The large language model and high-level vision model have achieved impressive performance improvements with large datasets and model sizes. However, low-level computer vision tasks, such as image dehaze and blur removal,\nstill rely on a small number of datasets and small-sized models, which generally leads to overfitting and local optima. Therefore, we propose a framework to integrate large-model prior into low-level computer vision\ntasks. Just as with the task of image segmentation, the degradation of haze is also texture-related. So we propose to detect gray-scale coding, network channel expansion, and pre-dehaze structures to integrate large-\nmodel prior knowledge into any low-level dehazing network. We demonstrate the effectiveness and applicability of large models in guiding low-level visual tasks through different datasets and algorithms comparison\nexperiments. Finally, we demonstrate the effect of grayscale coding, network channel expansion, and recurrent network structures through ablation experiments. Under the conditions where additional data and training\nresources are not required, we successfully prove that the integration of large-model prior knowledge will improve the dehaze performance and save training time for low-level visual tasks.*\n\nURL: http://arxiv.org/abs/2306.15870v1\n\nAuthors: Zheyan Jin, Shiqi Chen, Yueting Chen, Zhihai Xu, Huajun Feng\n\n_**Published on**_ : **June 28, 2023 - 02:02 AM**\n\nUpdated on: June 28, 2023 - 02:02 AM\n\n*arXiv ID*: **2306.15870v1**\n\nPrimary Category:  _**cs.CV**_\n\n\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import arxiv\n",
        "import feedparser\n",
        "import urllib.parse\n",
        "import textwrap\n",
        "import time\n",
        "from IPython.core.display import display, Markdown\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Function to extract arXiv ID from link\n",
        "def extract_arxiv_id(link):\n",
        "    pattern = r'abs/(\\d+\\.\\d+v\\d+)'\n",
        "    match = re.search(pattern, link)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Define your search query\n",
        "search_keyword = input(\"Enter the search keyword: \")\n",
        "search_query = f'ti:\"{search_keyword}\"'\n",
        "\n",
        "# Base URL for the arXiv API\n",
        "base_url = 'http://export.arxiv.org/api/query?'\n",
        "\n",
        "# Define the parameters for the search\n",
        "params = urllib.parse.urlencode({\n",
        "    'search_query': search_query,\n",
        "    'start': 0,\n",
        "    'max_results': 30,\n",
        "    'sortBy': 'submittedDate',\n",
        "    'sortOrder': 'descending'\n",
        "})\n",
        "\n",
        "\n",
        "# Form the complete URL for the request\n",
        "url = base_url + params\n",
        "\n",
        "# Make the request to the arXiv API and parse the response\n",
        "response = feedparser.parse(url)\n",
        "\n",
        "# Check if the response has entries\n",
        "if 'entries' in response:\n",
        "    # Initialize the counter\n",
        "    article_number = 1\n",
        "\n",
        "    # Loop over the entries in the response and display the detailed information of each paper\n",
        "    for entry in response.entries:\n",
        "        # Parse the dates\n",
        "        published_date = datetime.strptime(entry.published, '%Y-%m-%dT%H:%M:%SZ')\n",
        "        updated_date = datetime.strptime(entry.updated, '%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "        # Format 1: Month Day, Year - Hour:Minute AM/PM\n",
        "        published_str_1 = published_date.strftime('%B %d, %Y - %I:%M %p')\n",
        "        updated_str_1 = updated_date.strftime('%B %d, %Y - %I:%M %p')\n",
        "\n",
        "        # Adjust titles to a new line with Markdown heading\n",
        "        cleaned_title = entry.title.replace( '\\n' ,  '\\n#### ')\n",
        "\n",
        "        arxiv_id = extract_arxiv_id(entry.link)\n",
        "\n",
        "        output = f\"\"\"\n",
        "\n",
        "#### {article_number}) Title: {cleaned_title}\n",
        "\n",
        "*{textwrap.fill(entry.summary, width=220)}*\n",
        "\n",
        "URL: {entry.link}\n",
        "\n",
        "Authors: {', '.join(author.name for author in entry.authors)}\n",
        "\n",
        "_**Published on**_ : **{published_str_1}**\n",
        "\n",
        "Updated on: {updated_str_1}\n",
        "\n",
        "*arXiv ID*: **{arxiv_id}**\n",
        "\n",
        "Primary Category:  _**{entry.arxiv_primary_category['term']}**_\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "        display(Markdown(output))\n",
        "\n",
        "        # Increment the counter\n",
        "        article_number += 1\n",
        "\n",
        "        # Sleep for 3 seconds to avoid overloading the server\n",
        "        time.sleep(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Papers in Bulk using `Arxiv Id` and downloading them locally as a single `.zip` file  \n",
        "\n",
        "You can see the **Arxiv ID** attribute printed out at the end of each result you can use this to quickly downloads papers , just paste one or more **arxiv-ID's**  in the cell below to download the papers to ``arxiv-papers`` directory in ``content``"
      ],
      "metadata": {
        "id": "ICNQMGMuu6g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Arxiv-dl (installed earlier ) to download papers"
      ],
      "metadata": {
        "id": "Lh3yRCjG3vHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/arxiv-paper\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aUP6Klz2TgF",
        "outputId": "31b86102-d18f-4997-baf9-d3d3aa6f994d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arxiv-paper  arxiv-paper.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####_**Download papers with markdown files and other items**_\n",
        "- run this cell and check the `arxiv-paper` directory at `content/arxiv-paper` to see your downloads\n",
        "- you can add more then one id in the command and chain them togather like this\n",
        "```\n",
        "!paper  id 1 id 2  .. -d /location/dir/\n",
        "```\n",
        "like this\n",
        "```\n",
        "!paper `2302.24532v1` `2342.3532v2` -d /content/arxiv paper\n",
        "```\n",
        "to download more then one paper at the directory `arxiv-paper`\n",
        "\n",
        "## Run cell below to donwload papers  you put after !paper\n"
      ],
      "metadata": {
        "id": "bfmkQkid4-ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!paper  2306.16623v1 -d /content/arxiv-paper"
      ],
      "metadata": {
        "id": "1M9Gd_GO45kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####*If* you want to download just the **pdfs** of papers and not the markdown notes  with them then use `-p` flag at the end of the command like here\n",
        "\n",
        "\n",
        "- run this cell and check the `arxiv-paper` directory at `content/arxiv-paper` to see your downloads\n",
        "\n",
        "how to download in bulk :\n",
        "```\n",
        "!paper  id 1 id 2  .. -d /location/dir/ -p\n",
        "```\n",
        "\n",
        "like this\n",
        "```\n",
        "!paper `2302.24532v1` `2342.3532v2` -d /content/arxiv paper -p\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "oKhKcolx39pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!paper  2306.16623v1 -d /content/arxiv-paper  -p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Ubn2AMG48f",
        "outputId": "a850d4cb-4466-4ca4-8614-672f0b38388c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[arxiv-dl] (version: 1.1.4)\n",
            "[1/1] >>> 2306.16623v1\n",
            "\u001b[32m[Processing] Retrieving paper metadata...\u001b[0m\n",
            "\u001b[32m[Done] Paper PDF already exists at: \"/content/arxiv-paper/2306.16623v1_The_Segment_Anything_Model_SAM_for_Remote_Sensing_Applications_From_Zero_to_One_Shot.pdf\"\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Single command to zip** ``arxiv-paper `` folder and make a ``arxiv-paper.zip`` file in `/content` directory"
      ],
      "metadata": {
        "id": "4pasewwHB0Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r arxiv-paper.zip arxiv-paper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB3YmqpN7mS4",
        "outputId": "f4c3c729-bcad-478a-cf11-7149ea05ec90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: arxiv-paper/ (stored 0%)\n",
            "  adding: arxiv-paper/000_Paper_List.json (deflated 55%)\n",
            "  adding: arxiv-paper/2306.16623v1_The_Segment_Anything_Model_SAM_for_Remote_Sensing_Applications_From_Zero_to_One_Shot.pdf (deflated 1%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Run this cell to download the .zip you just created to your local computer**"
      ],
      "metadata": {
        "id": "DhOOxZipBkQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('arxiv-paper.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YT0mRcva7z3I",
        "outputId": "95208fde-c9e0-4ead-bf92-f7ff5453044e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8988acbb-f430-417d-bd72-edd9ec1cdeb3\", \"arxiv-paper.zip\", 50123611)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
